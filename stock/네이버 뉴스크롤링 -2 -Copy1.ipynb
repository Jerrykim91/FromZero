{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def MakeNouns( pathName, fileName, Contents ):\n",
    "#     \"\"\"\n",
    "#     만약 Kkma가 설치 되어 있다면 상관없음 \n",
    "#     없으면 설치 권장 \n",
    "#     \"\"\"\n",
    "#     from konlpy.tag import Kkma\n",
    "#     kkma = Kkma()\n",
    "#     path = pathName + fileName\n",
    "#     df_origin  = pd.read_csv( path + '.csv', index_col=False)\n",
    "#     df = df_origin\n",
    "    \n",
    "#     df['Keyword'] = None  # 초기값 세팅 \n",
    "    \n",
    "#     for idx, sen in tqdm(enumerate(Contents)):\n",
    "#         chge   = re.compile('[가-힣. ]+').findall(sen) # 정규식 1차 처리\n",
    "#         nouns  = kkma.nouns( ' '.join(chge) ) # 띄워쓰기 후 명사 추출  \n",
    "#         df['Keyword'][idx] = nouns\n",
    "        \n",
    "#     fullDstName =  path + '_Keyword.csv'\n",
    "#     SaveFile(df, fullDstName,'w', False, 'utf-8-sig' )    \n",
    "    \n",
    "#     return print('Finish !')\n",
    "\n",
    "\n",
    "# def TotalStr(Bunch):\n",
    "#     \"\"\"\n",
    "#     # 리스트안의 중복된 값의 개수\n",
    "#     string 중복값 생성 \n",
    "#     \"\"\"\n",
    "#     import operator\n",
    "\n",
    "#     oneWords = dict()\n",
    "#     for lst in Bunch:\n",
    "#         try: oneWords[lst] += 1\n",
    "#         except: oneWords[lst]=1\n",
    "\n",
    "#     oneWord = sorted(oneWords.items(),reverse=True, key=operator.itemgetter(1))\n",
    "    \n",
    "#     return oneWord\n",
    "\n",
    "\n",
    "# # 자동화 함수 - 구글드라이버를 통해 진행\n",
    "# def GetConfirmToken(response):\n",
    "#     \"\"\"\n",
    "#     # 토큰 받기 \n",
    "#     \"\"\"\n",
    "#     for key, val in response.cookies.items():\n",
    "#         if key.startswith(\"download_warning\"): #  특정 문자로 시작하는지의 여부\n",
    "#             return val\n",
    "#     return None\n",
    "\n",
    "\n",
    "# def SaveResContent(response, destination ):\n",
    "#     \"\"\"\n",
    "#     한 번에 하나씩 데이터를 읽어 Chunk라는 덩어리를 만든 후  \n",
    "#     Chunk 단위로 트랜잭션 -> chunkSize별로 묶어서 처리\n",
    "#     \"\"\"\n",
    "#     chunkSize = 32768\n",
    "#     with open(destination, \"wb\") as f:\n",
    "#         for chunk in response.iter_content(chunkSize):\n",
    "#             if chunk : \n",
    "#                 f.write(chunk)   \n",
    "\n",
    "# def DownloadFunc(id_, destination):\n",
    "#     \"\"\"\n",
    "#     # 구글 드라이버를 통해 데이터 자동 다운로드 \n",
    "#     # id_ : 아이디 -> 제공 \n",
    "#     # destination : 경로 = pathName\n",
    "#     \"\"\"\n",
    "#     baseUrl = \"https://docs.google.com/uc?export=download\"\n",
    "#     session = requests.Session()\n",
    "#     res   = session.get(baseUrl, params = { 'id' : id_ }, stream = True )\n",
    "#     token = GetConfirmToken(res)\n",
    "#     if token:\n",
    "#         params = { 'id' : id_, 'confirm' : token }\n",
    "#         res    = session.get( baseUrl, params = params, stream = True )\n",
    "#         print(f'성공 : {res}')\n",
    "    \n",
    "#     print('다운로드 시작')    \n",
    "#     baseName = res.headers['Content-Disposition'].split(';')[1].split('filename=')[1].replace('\\\"', '')\n",
    "#     fullFileName = path.join( destination, baseName )\n",
    "#     SaveResContent( res, fullFileName )\n",
    "#     print('진행 중 ')\n",
    "#     return fullFileName\n",
    "\n",
    "\n",
    "\n",
    "# def DownloadFromGoogleDrive(fileId, destination):\n",
    "#     \"\"\"\n",
    "#     # 구글 데이터 다운로드   \n",
    "#     fileId   : id 코드 \n",
    "#     pathName : 다운로드 경로 \n",
    "#     \"\"\"\n",
    "#     cnt = 0 \n",
    "#     filenameList = list()\n",
    "#     os.makedirs(destination, exist_ok = True)\n",
    "#     try:\n",
    "#         if len(fileId) == 33 and type(fileId) == str :\n",
    "#             download_func = partial( DownloadFunc, destination = pathName )\n",
    "#             print('다운로드 진행중!')\n",
    "#             fileName = DownloadFunc( id_ = fileId, destination = pathName )\n",
    "#             print(f'{fileName} 완료!')\n",
    "#             filenameList.append(fileName)   \n",
    "\n",
    "#         elif type(fileId) == list :\n",
    "#             for key in tqdm(fileId):\n",
    "#                 cnt += 1 \n",
    "#                 download_func = partial( DownloadFunc, destination = pathName )\n",
    "#                 print('다운로드 진행중!')\n",
    "#                 fileName = DownloadFunc( id_ = key, destination = pathName )\n",
    "#                 print(f'{cnt}번째 {fileName} 완료!')\n",
    "#                 filenameList.append(fileName)   \n",
    "#                 print('='*60)\n",
    "#         else:\n",
    "#             print(' 33자리 Id 코드가 필요합니다. 확인해 주세요!')    \n",
    "                \n",
    "#     except Exception as err:\n",
    "#         print(f'{err} : 에러 발생')\n",
    "        \n",
    "#     return filenameList\n",
    "\n",
    "\n",
    "# def Unzip(filenameList):\n",
    "#     \"\"\"\n",
    "#     # 압축 해제\n",
    "#     \"\"\"\n",
    "#     zipFileNameList = [ pathName for pathName in filenameList if pathName.endswith('.zip')]\n",
    "#     for file in tqdm(zipFileNameList):\n",
    "#         print(f'{file} 진행중!')\n",
    "#         with zipfile.ZipFile(file) as targetZip:\n",
    "#             destPath = path.splitext(file)[0]\n",
    "#             os.makedirs(destPath, exist_ok=True)\n",
    "#             targetZip.extractall(destPath)\n",
    "#             print(f'{destPath} 완료!')\n",
    "#         print(f'.zip 파일삭제 {os.remove(file)}')      \n",
    "\n",
    "\n",
    "# def SaveFile( df, fullDstName,  Mode, index, encoding ):\n",
    "#     \"\"\"\n",
    "#     df = 데이터 프레임\n",
    "#     Mode = 만약 덮어쓰기가 있다면 a , 그냥 생성 w\n",
    "#     fullDstName =  path + 파일 확장자(확장자 주의 요말)\n",
    "#     index = False\n",
    "#     encoding = 인코딩 \n",
    "#     header -> 덮어쓰기에만 적용\n",
    "#     # 2018, 2019, 2020\n",
    "#     \"\"\"\n",
    "#     if fullDstName.split('.')[1] == 'csv' :\n",
    "#         if Mode == 'a' : \n",
    "#             # 파일 여부 확인 후 생성 \n",
    "#             if not os.path.exists( fullDstName ):\n",
    "#                 df.to_csv( fullDstName , index = index, mode = 'w', encoding = encoding )\n",
    "#             else: \n",
    "#                 df.to_csv( fullDstName , index = index, mode = 'a', encoding = encoding, header = False ) \n",
    "#         else:\n",
    "#             df.to_csv( fullDstName , index = index, mode = 'w', encoding = encoding )\n",
    "\n",
    "        \n",
    "#     elif fullDstName.split('.')[1] == 'xlsx' :\n",
    "#         if Mode == 'a' : \n",
    "#             # 파일 여부 확인 후 생성 \n",
    "#             if not os.path.exists( fullDstName ):\n",
    "#                 df.to_xlsx( fullDstName , index = index, mode = 'w', encoding = encoding )\n",
    "#             else: \n",
    "#                 df.to_xlsx( fullDstName , index = index, mode = 'a', encoding = encoding, header = False )\n",
    "#         else:\n",
    "#             df.to_xlsx( fullDstName , index = index, mode = 'w', encoding = encoding )\n",
    "#     else:\n",
    "#         print('Please check your file path! Only, xlsx and csv can use')\n",
    "\n",
    "\n",
    "\n",
    "# def Main( y, m, d, txt, pathName, fileName):\n",
    "#     \"\"\"\n",
    "#     # 크롤링 메인 함수 - 1일 기준 \n",
    "#     y = Year m = Month, d = Day, txt = Keyword, pathName = 파일 위치, fileName = output 될 메인파일이름 \n",
    "#     output 파일은 이런 형식으로 {name}_{y}-{m}-{d}' \n",
    "#     \"\"\"\n",
    "#     page    = 1\n",
    "#     dic     = {}\n",
    "    \n",
    "#     mkDir( pathName, fileName ) # 디렉토리 생성 \n",
    "    \n",
    "#     m = StrDate(m) # Date Type\n",
    "#     d = StrDate(d)\n",
    "        \n",
    "#     Running = True \n",
    "#     while Running:\n",
    "        \n",
    "#         targetUrl      = PageUrl( txt, y, m, d, page )       # targetUrl \n",
    "#         Titles, Posts  = BasicContents( targetUrl )          # Title, Company name  \n",
    "#         CurrPagesStep, TitPages  = MaxPageCunt( targetUrl )  # Each Count, Total Pages Count\n",
    "#         print(targetUrl) \n",
    "#         try:\n",
    "#             Titles, Posts = BasicContents( targetUrl )\n",
    "#         except:\n",
    "#             Titles = 'NaN'\n",
    "#             Posts  = 'NaN'\n",
    "\n",
    "#         date = str(y)+'-'+str(m)+'-'+str(d)\n",
    "        \n",
    "#         dic['Title'] = Titles\n",
    "#         dic['Post']  = Posts\n",
    "#         dic['Date']  = date\n",
    "        \n",
    "#         df = pd.DataFrame( dic ) # 데이터 프레임 저장 \n",
    "        \n",
    "#         fullDstName = pathName+'%s.csv'% f'{fileName}/{fileName}_{date}'\n",
    "#         SaveFile(df, fullDstName, 'a', False,  'utf-8-sig') # df, fullDstName,  Mode, index, encoding\n",
    "#         print( '='*20, f'{page}','='*20 )\n",
    "#         page += 10 l\n",
    "        \n",
    "#         #  종료 옵션 \n",
    "#         if page == 4001 :  # Crawling max page 4000!\n",
    "#             Running = False\n",
    "#         elif TitPages == 0 :\n",
    "#             Running = False\n",
    "#         elif page == (TitPages*10)+1:\n",
    "#             Running = False\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os, glob, time, math\n",
    "import pandas as pd\n",
    "\n",
    "# 크롤링\n",
    "import requests\n",
    "import urllib.request\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveFile( df, fullDstName,  Mode, index, encoding ):\n",
    "    \"\"\"\n",
    "    df = 데이터 프레임\n",
    "    Mode = 만약 덮어쓰기가 있다면 a , 그냥 생성 w\n",
    "    fullDstName =  path + 파일 확장자(확장자 주의 요말)\n",
    "    index = False\n",
    "    encoding = 인코딩 \n",
    "    header -> 덮어쓰기에만 적용\n",
    "    # 2018, 2019, 2020\n",
    "    \"\"\"\n",
    "    if fullDstName.split('.')[1] == 'csv' :\n",
    "        if Mode == 'a' : \n",
    "            # 파일 여부 확인 후 생성 \n",
    "            if not os.path.exists( fullDstName ):\n",
    "                df.to_csv( fullDstName , index = index, mode = 'w', encoding = encoding )\n",
    "            else: \n",
    "                df.to_csv( fullDstName , index = index, mode = 'a', encoding = encoding, header = False ) \n",
    "        else:\n",
    "            df.to_csv( fullDstName , index = index, mode = 'w', encoding = encoding )\n",
    "\n",
    "        \n",
    "    elif fullDstName.split('.')[1] == 'xlsx' :\n",
    "        if Mode == 'a' : \n",
    "            # 파일 여부 확인 후 생성 \n",
    "            if not os.path.exists( fullDstName ):\n",
    "                df.to_xlsx( fullDstName , index = index, mode = 'w', encoding = encoding )\n",
    "            else: \n",
    "                df.to_xlsx( fullDstName , index = index, mode = 'a', encoding = encoding, header = False )\n",
    "        else:\n",
    "            df.to_xlsx( fullDstName , index = index, mode = 'w', encoding = encoding )\n",
    "    else:\n",
    "        print('Please check your file path! Only, xlsx and csv can use')\n",
    "\n",
    "        \n",
    "def Soup( targetUrl ):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        str = requests.get( targetUrl , headers={'User-Agent':'Mozilla/5.0'} )\n",
    "    except:\n",
    "        str = requests.get( targetUrl )\n",
    "    soup    = BeautifulSoup( str.content , 'lxml' ) # lxml    \n",
    "    \n",
    "    return soup   \n",
    "\n",
    "        \n",
    "        \n",
    "def SubContents( targetUrl ):\n",
    "    \"\"\"\n",
    "    SubContents\n",
    "    #News address  = targetUrl\n",
    "    \"\"\"\n",
    "    soup    = Soup( targetUrl )\n",
    "    urlNew  = [i['href'] for i in soup.select( '.news .type01 li dt a')]\n",
    "    \n",
    "    return urlNew   \n",
    "\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# def Subtxt( targetUrl ):\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     soup    = Soup( targetUrl )\n",
    "#     cont    = [i.text for i in soup.select('#content')]\n",
    "    \n",
    "#     return cont  \n",
    "\n",
    "\n",
    "def Subtxt( targetUrl ):\n",
    "    \n",
    "    box=list()\n",
    "    for url in targetUrl:\n",
    "        \n",
    "        id_result = {'Code': ['content', 'article', 'CmAdContent', 'articleWrap']}\n",
    "        punc = '[!\"#$%&\\'()*+,-./:;<=>?[\\]^_`{|}~“”·]'\n",
    "        soup    = Soup(url) # func\n",
    "\n",
    "        try:\n",
    "            for i in id_result['Code']:\n",
    "                cont    = [re.sub(punc,'',i.text).strip().replace('\\n','') for i in soup.select('#'+i)]\n",
    "                ## 여기 빈값을 찾아라 \n",
    "                if len(cont) <= 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    return url\n",
    "        \n",
    "            box.append(cont)\n",
    "            return box\n",
    "        \n",
    "        except:\n",
    "            print('missUrl', targetUrl)\n",
    "    \n",
    "###############################################################################\n",
    "        \n",
    "def PageUrl( txt, y, m, d, page ):\n",
    "    \"\"\"\n",
    "    # Page assignment\n",
    "    from urllib.parse import quote\n",
    "    page = '11 단위 수치정보' -> 11,21,31 이런식 \n",
    "    \"\"\"\n",
    "    keyword    = quote(txt)\n",
    "    mainUrl    = f'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={keyword}'\n",
    "\n",
    "    if page == 1 :\n",
    "        #  page data\n",
    "        paramUrl  = f'&sm=tab_opt&sort=0&photo=0&field=0&reporter_article=&pd=3&ds={y}.{m}.{d}&de={y}.{m}.{d}&docid=&nso=so%3Ar%2Cp%3Afrom{y}{m}{d}to{y}{m}{d}%2Ca%3Aall&mynews=0&refresh_start=0&related=0'\n",
    "        targetUrl = mainUrl + paramUrl\n",
    "        return targetUrl\n",
    "\n",
    "    else:\n",
    "        # Non-page data\n",
    "        paramUrl  = f'&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3&ds={y}.{m}.{d}&de={y}.{m}.{d}&docid=&nso=so:r,p:from{y}{m}{d}to{y}{m}{d},a:all&mynews=0&cluster_rank=32&start={page}&refresh_start=0'\n",
    "        targetUrl = mainUrl + paramUrl\n",
    "        return targetUrl\n",
    "\n",
    "\n",
    "    \n",
    "def BasicContents( targetUrl ):\n",
    "    \"\"\"\n",
    "    언론사 그리고 큐스 타이틀만 가지고 오기 \n",
    "    News address  = targetUrl\n",
    "    \"\"\"\n",
    "    soup    = Soup( targetUrl )\n",
    "\n",
    "    newsTitles = soup.select( '.news .type01 li dt a[title]' )\n",
    "    step       = soup.select( '.news .type01 li dd ._sp_each_source' )\n",
    "    \n",
    "    # 리스트에 담아서 리턴\n",
    "    cmpy = [ tit['title'] for tit in newsTitles ]          # 언론사 \n",
    "    txt  = [ s.text.split('언론사 선정')[0] for s in step] # 뉴스 타이틀\n",
    "    \n",
    "    return cmpy, txt   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def MaxPageCunt( targetUrl ):\n",
    "    \"\"\"\n",
    "    import math \n",
    "    # Full viewable page  -> MaxPageCunt( target_url ) \n",
    "    \"\"\" \n",
    "    soup    = Soup( targetUrl )\n",
    "    maxPages = soup.find('div',{'class':'title_desc all_my'}) # Total page  \n",
    "    \n",
    "    if maxPages != None:\n",
    "        maxPages = maxPages.find('span').text.split('/')[1].replace('건','').replace(',','').replace(' ','')\n",
    "        print('maxPages',maxPages )\n",
    "\n",
    "        # The number of items on the current page\n",
    "        CurrPagesStep = soup.find('div',{'class':'title_desc all_my'})\n",
    "        CurrPagesStep = CurrPagesStep.find('span').text.split('/')[0].split('-')[1].replace(' ','').replace(',','')\n",
    "\n",
    "        CurrPagesStep = int( CurrPagesStep )\n",
    "        TitPages      = int( maxPages )/4000   # Total Page -> More than 4000 fix it  \n",
    "\n",
    "        # Page count\n",
    "        if TitPages >= 1 :  # More than 4000  \n",
    "            maxPages = 4000\n",
    "            TitPages = int(int(maxPages)/10)\n",
    "\n",
    "        else: # 4000 or less \n",
    "            TitPages = math.ceil(int(maxPages)/10)\n",
    "        return CurrPagesStep, TitPages \n",
    "    \n",
    "    else:\n",
    "        CurrPagesStep = 0\n",
    "        TitPages      = 0\n",
    "        return CurrPagesStep, TitPages \n",
    "    \n",
    "     \n",
    "def StrDate(num):\n",
    "    \"\"\"\n",
    "    # date type change \n",
    "    \"\"\"\n",
    "    if num >= 10:\n",
    "        num = str(num)\n",
    "    else:\n",
    "        num = \"0\"+str(num)\n",
    "    return num\n",
    "\n",
    "\n",
    "def mkDir(pathName, fileName):\n",
    "    \"\"\"\n",
    "    pathName = 'C:/data/'      # 경로\n",
    "    fileName = '이름'          # 이름 \n",
    "    Save Folder Name = pathName, dirname\n",
    "    \"\"\"\n",
    "    path = pathName + fileName\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "        targetDir = glob.glob(path)\n",
    "        print('Folder Created')\n",
    "    else:\n",
    "        targetDir = glob.glob(path)\n",
    "        print('Folder already exist')\n",
    "        \n",
    "    return path   \n",
    "\n",
    "\n",
    "\n",
    "def Main( y, m, d, txt, pathName, fileName):\n",
    "    \"\"\"\n",
    "    # 크롤링 메인 함수 - 1일 기준 \n",
    "    y = Year m = Month, d = Day, txt = Keyword, pathName = 파일 위치, fileName = output 될 메인파일이름 \n",
    "    output 파일은 이런 형식으로 {name}_{y}-{m}-{d}' \n",
    "    \"\"\"\n",
    "    page    = 1\n",
    "    dic     = {}\n",
    "    \n",
    "    mkDir( pathName, fileName ) # 디렉토리 생성 \n",
    "    \n",
    "    m = StrDate(m) # Date Type\n",
    "    d = StrDate(d)\n",
    "        \n",
    "    Running = True \n",
    "    while Running:\n",
    "        \n",
    "        targetUrl      = PageUrl( txt, y, m, d, page )       # targetUrl \n",
    "        Titles, Posts  = BasicContents( targetUrl )          # Title, Company name  \n",
    "        \n",
    "        subUrl         = SubContents(targetUrl)\n",
    "        for u in subUrl:\n",
    "            dic['SubContent'] = Subtxt(u)\n",
    "        \n",
    "        CurrPagesStep, TitPages  = MaxPageCunt( targetUrl )  # Each Count, Total Pages Count\n",
    "        print(targetUrl) \n",
    "        try:\n",
    "            Titles, Posts = BasicContents( targetUrl )\n",
    "        except:\n",
    "            Titles = 'NaN'\n",
    "            Posts  = 'NaN'\n",
    "\n",
    "        date = str(y)+'-'+str(m)+'-'+str(d)\n",
    "        \n",
    "        dic['Title'] = Titles\n",
    "        dic['Post']  = Posts\n",
    "        dic['Date']  = date\n",
    "        \n",
    "        df = pd.DataFrame( dic ) # 데이터 프레임 저장 \n",
    "        \n",
    "        fullDstName = pathName+'%s.csv'% f'{fileName}/{fileName}_{date}'\n",
    "        SaveFile(df, fullDstName, 'a', False,  'euc-kr') # df, fullDstName,  Mode, index, encoding\n",
    "        print( '='*20, f'{page}','='*20 )\n",
    "        page += 10 \n",
    "        \n",
    "        #  종료 옵션 \n",
    "        if page == 4001 :  # Crawling max page 4000!\n",
    "            Running = False\n",
    "        elif TitPages == 0 :\n",
    "            Running = False\n",
    "        elif page == (TitPages*10)+1:\n",
    "            Running = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "def MergeData( pathName, fileName ):\n",
    "    \"\"\"\n",
    "    # 일단위로 저장 된 데이터를 병합 \n",
    "    import pandas as pd\n",
    "    import glob, os\n",
    "    pathName = 'C:/data/' \n",
    "    fileName = '이름'\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    dataList = list()\n",
    "    path = mkDir( pathName, fileName ) # Dir check func if Do not have dir make it!\n",
    "    \n",
    "    for pth in tqdm(glob.glob( path +'/*')):\n",
    "        origin  = pd.read_csv( pth, index_col = False )\n",
    "        cnt = cnt + len(origin)\n",
    "        dataList.append(origin)\n",
    "        \n",
    "    CatList= pd.concat( dataList, axis=0, ignore_index = True )\n",
    "    # data folder dir  Save csv file\n",
    "    fullDstName =  path +'.csv'\n",
    "    SaveFile(CatList, fullDstName,'w', False, 'euc-kr')\n",
    "#     SaveFile(CatList, fullDstName,'w', False, 'utf-8-sig')\n",
    "    newPath = glob.glob(pathName+'/*')\n",
    "    print(cnt)\n",
    "    \n",
    "    return newPath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 진행코드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1\n",
    "d = 1\n",
    "y = 2020\n",
    "\n",
    "pathName = 'C:/TmP/' # 사용자가 원하는 경로\n",
    "fileName     = 'test' \n",
    "txt          = '사회문제' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targetUrl = 'http://yna.kr/AKR20200101039051074?did=1195m'\n",
    "# print(targetUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exist\n"
     ]
    },
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL 'h': No schema supplied. Perhaps you meant http://h?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d5018e8af3b8>\u001b[0m in \u001b[0;36mSoup\u001b[1;34m(targetUrl)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtargetUrl\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'User-Agent'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'Mozilla/5.0'\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    515\u001b[0m         )\n\u001b[1;32m--> 516\u001b[1;33m         \u001b[0mprep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m         p.prepare(\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMissingSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMissingSchema\u001b[0m: Invalid URL 'h': No schema supplied. Perhaps you meant http://h?",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-daf22e391706>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mTitles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPosts\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mBasicContents\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtargetUrl\u001b[0m \u001b[1;33m)\u001b[0m          \u001b[1;31m# Title, Company name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0msubUrl\u001b[0m         \u001b[1;33m=\u001b[0m \u001b[0mSubContents\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtargetUrl\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mSubContent\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mSubtxt\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtargetUrl\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSubContent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-d5018e8af3b8>\u001b[0m in \u001b[0;36mSubtxt\u001b[1;34m(targetUrl)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mid_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'Code'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'article'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'CmAdContent'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'articleWrap'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mpunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'[!\"#$%&\\'()*+,-./:;<=>?[\\]^_`{|}~“”·]'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0msoup\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# func\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-d5018e8af3b8>\u001b[0m in \u001b[0;36mSoup\u001b[1;34m(targetUrl)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtargetUrl\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'User-Agent'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'Mozilla/5.0'\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtargetUrl\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0msoup\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;31m# lxml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    514\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         )\n\u001b[1;32m--> 516\u001b[1;33m         \u001b[0mprep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m         p.prepare(\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_native_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMissingSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhost\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMissingSchema\u001b[0m: Invalid URL 'h': No schema supplied. Perhaps you meant http://h?"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 크롤링 메인 함수 - 1일 기준 \n",
    "y = Year m = Month, d = Day, txt = Keyword, pathName = 파일 위치, fileName = output 될 메인파일이름 \n",
    "output 파일은 이런 형식으로 {name}_{y}-{m}-{d}' \n",
    "\"\"\"\n",
    "page    = 1\n",
    "dic     = {}\n",
    "dic_sub = {}\n",
    "mkDir( pathName, fileName ) # 디렉토리 생성 \n",
    "\n",
    "m = StrDate(m) # Date Type\n",
    "d = StrDate(d)\n",
    "\n",
    "Running = True \n",
    "while Running:\n",
    "\n",
    "    targetUrl      = PageUrl( txt, y, m, d, page )       # targetUrl \n",
    "    Titles, Posts  = BasicContents( targetUrl )          # Title, Company name  \n",
    "    subUrl         = SubContents( targetUrl )\n",
    "    SubContent     = Subtxt( targetUrl ) \n",
    "    print(SubContent)\n",
    "\n",
    "    CurrPagesStep, TitPages  = MaxPageCunt( targetUrl )  # Each Count, Total Pages Count\n",
    "    print(targetUrl) \n",
    "    try:\n",
    "        Titles, Posts = BasicContents( targetUrl )\n",
    "    except:\n",
    "        Titles = 'NaN'\n",
    "        Posts  = 'NaN'\n",
    "\n",
    "    date = str(y)+'-'+str(m)+'-'+str(d)\n",
    "\n",
    "    dic['Title'] = Titles\n",
    "    dic['Post']  = Posts\n",
    "    dic['Date']  = date\n",
    "#     dic['SubContent'] = box\n",
    "    \n",
    "    df = pd.DataFrame( dic ) # 데이터 프레임 저장 \n",
    "\n",
    "    fullDstName = pathName+'%s.csv'% f'{fileName}/{fileName}_{date}'\n",
    "    SaveFile(df, fullDstName, 'a', False,  'utf-8') # df, fullDstName,  Mode, index, encoding\n",
    "    print( '='*20, f'{page}','='*20 )\n",
    "    page += 10 \n",
    "\n",
    "    #  종료 옵션 \n",
    "    if page == 4001 :  # Crawling max page 4000!\n",
    "        Running = False\n",
    "    elif TitPages == 0 :\n",
    "        Running = False\n",
    "    elif page == (TitPages*10)+1:\n",
    "        Running = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punc = '[!\"#$%&\\'()*+,-./:;<=>?[\\]^_`{|}~“”·]'\n",
    "# re.sub(punc,'',p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Code': ['content', 'article', 'CmAdContent']}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Subtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_result['Code']= dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxPages 219\n",
      "https://search.naver.com/search.naver?where=news&sm=tab_jum&query=%EC%82%AC%ED%9A%8C%EB%AC%B8%EC%A0%9C&sm=tab_opt&sort=0&photo=0&field=0&reporter_article=&pd=3&ds=2020.01.01&de=2020.01.01&docid=&nso=so%3Ar%2Cp%3Afrom20200101to20200101%2Ca%3Aall&mynews=0&refresh_start=0&related=0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-15d6359405c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mMain\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileName\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-bfbf3b0bd63f>\u001b[0m in \u001b[0;36mMain\u001b[1;34m(y, m, d, txt, pathName, fileName)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mdic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdic\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;31m# 데이터 프레임 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[0mfullDstName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpathName\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'%s.csv'\u001b[0m\u001b[1;33m%\u001b[0m \u001b[1;34mf'{fileName}/{fileName}_{date}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    433\u001b[0m             )\n\u001b[0;32m    434\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         ]\n\u001b[1;32m--> 254\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    363\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arrays must all be same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "# path =  pathName +  dirPath[0] + \"/\" \n",
    "# pathName = 'C:/data/'  # 사용자 임의\n",
    "# 변수 : 사용자 설정 \n",
    "fileName     = 'test' \n",
    "txt          = '사회문제' \n",
    "start_month  = 1\n",
    "end_month    = 1\n",
    "year         = 2020\n",
    "\n",
    "# mkDir( pathName, fileName )   #  폴더 생성 만약 있으면 pass\n",
    "\n",
    "# 기간 + 크롤링(전체 기간을 돌리는 경우 break 제거해야합니다.)\n",
    "thirtyOne = [1,3,5,7,8,10,12] \n",
    "for mth in range(start_month, end_month+1):\n",
    "    if mth == 2 : # 2월 \n",
    "        for dt in range(1,30):\n",
    "            Main( year, mth, dt, txt, pathName, fileName ) # 크롤링 코드 1일 \n",
    "            break\n",
    "    elif mth not in thirtyOne : \n",
    "        for dt in range(1,31):\n",
    "            Main( year, mth, dt, txt, pathName, fileName )\n",
    "            break\n",
    "    else:\n",
    "        for dt in range(1,32):\n",
    "            Main( year, mth, dt, txt, pathName, fileName )\n",
    "            break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(targetUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = SubContents(targetUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.donga.com/news/article/all/20200902/102750395/1\n",
      "http://www.fnnews.com/news/202009011747032336\n",
      "http://www.jbnews.com/news/articleView.html?idxno=1304965\n",
      "http://www.inews24.com/view/1295321\n",
      "https://www.news1.kr/articles/?4045882\n",
      "http://star.mk.co.kr/new/view.php?mc=ST&year=2020&no=905412\n",
      "http://www.breaknews.com/752724\n",
      "http://www.newswatch.kr/news/articleView.html?idxno=50774\n",
      "https://platum.kr/archives/147802\n",
      "http://www.kukinews.com/newsView/kuk202008310170\n"
     ]
    }
   ],
   "source": [
    "for i in SubContents(targetUrl):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.donga.com/news/article/all/20200902/102750395/1'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Subtxt( targetUrl ):\n",
    "    \n",
    "    box=list()\n",
    "    for url in targetUrl:\n",
    "        \n",
    "        id_result = {'Code': ['content', 'article', 'CmAdContent', 'articleWrap']}\n",
    "        punc = '[!\"#$%&\\'()*+,-./:;<=>?[\\]^_`{|}~“”·]'\n",
    "        soup    = Soup(url) # func\n",
    "\n",
    "        try:\n",
    "            for i in id_result['Code']:\n",
    "                cont    = [re.sub(punc,'',i.text).strip().replace('\\n','') for i in soup.select('#'+i)]\n",
    "                ## 여기 빈값을 찾아라 \n",
    "                if len(cont) <= 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    return url\n",
    "        \n",
    "            box.append(cont)\n",
    "            return box\n",
    "        \n",
    "        except:\n",
    "            print('missUrl', targetUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
