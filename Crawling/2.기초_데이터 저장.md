<br>

#### 1. Crawling 기초 - 데이터 저장

<br>

# 데이터 관리 

<br>
 
## 1. 저장

<br>

```py

```

<br>




## 2.

<br>

```py



```

<br>



## 3.

<br>

```py

```

<br>



## 4. 데이터 병합 

판다스와 for 문을 이용해 데이터 병합

<br>

```py

pathName = 'C:\data\%s' 

dataList = list()
cnt = 0
# glob 경로를 추출 -> 경로를 read_csv
for pth in glob.glob(pathName%'Camp'+'/*'):
    # read_csv를 이용해 파일을 읽어서 변수에 담는다 
    origin  = pd.read_csv( pth, index_col=False)
    cnt = cnt + len(origin) # 데이터 검증을 위한 누적코드
    print(cnt)
    dataList.append(origin)

# 리스트에 담긴 데이터프레임을 concat을 이용해 병합한후 csv로 저장 
CatList= pd.concat(dataList,axis=0, ignore_index=True)
# axis=0은 수직, axis=1은 수평으로 병합
# ignore_index=True는 순서를 무시하고 순서대로 정렬
CatList.to_csv('C:/data/%s.csv'% f'Camp', index=False, mode='w', encoding='utf-8')

```

<br>





<br>

---

<br>

## Reference <br>

- 파이썬 코딩도장 &nbsp; : &nbsp;<https://dojang.io/> <br>

<br>
<br>

## Practice makes perfect! <br>

- [내용](주소)